diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 408a9749..94272215 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -14,6 +14,7 @@ from tensorflow.python.ops import functional_ops
 from tensorflow.python.ops import ctc_ops as ctc
 from .common import floatx, epsilon, image_data_format
 
+import sys
 import functools
 import threading
 
@@ -1203,7 +1204,9 @@ def update(x, new_x):
     # Returns
         The variable `x` updated.
     """
-    return tf_state_ops.assign(x, new_x)
+    op = tf_state_ops.assign(x, new_x)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -1217,7 +1220,9 @@ def update_add(x, increment):
     # Returns
         The variable `x` updated.
     """
-    return tf_state_ops.assign_add(x, increment)
+    op = tf_state_ops.assign_add(x, increment)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -1231,7 +1236,9 @@ def update_sub(x, decrement):
     # Returns
         The variable `x` updated.
     """
-    return tf_state_ops.assign_sub(x, decrement)
+    op = tf_state_ops.assign_sub(x, decrement)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -2880,6 +2887,7 @@ def get_variable_shape(x):
     return int_shape(x)
 
 
+@symbolic
 def print_tensor(x, message=''):
     """Prints `message` and the tensor value when evaluated.
 
@@ -2899,8 +2907,9 @@ def print_tensor(x, message=''):
     # Returns
         The same tensor `x`, unchanged.
     """
-    # TODO
-    return tf.Print(x, [x], message)
+    op = tf.print(message, x, output_stream=sys.stdout)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 # GRAPH MANIPULATION
diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py
index 141c6b9c..1f2789e9 100644
--- a/keras/backend/theano_backend.py
+++ b/keras/backend/theano_backend.py
@@ -1378,8 +1378,7 @@ def get_variable_shape(x):
 
 
 def print_tensor(x, message=''):
-    """Print the message and the tensor when evaluated and return the same
-    tensor.
+    """Print the message & the tensor when evaluated & return the same tensor.
     """
     p_op = Print(message)
     return p_op(x)
diff --git a/keras/initializers.py b/keras/initializers.py
index aea81b5c..eff00482 100644
--- a/keras/initializers.py
+++ b/keras/initializers.py
@@ -80,8 +80,11 @@ class RandomNormal(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.random_normal(shape, self.mean, self.stddev,
-                               dtype=dtype, seed=self.seed)
+        x = K.random_normal(shape, self.mean, self.stddev,
+                            dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -108,8 +111,11 @@ class RandomUniform(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.random_uniform(shape, self.minval, self.maxval,
-                                dtype=dtype, seed=self.seed)
+        x = K.random_uniform(shape, self.minval, self.maxval,
+                             dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -141,8 +147,11 @@ class TruncatedNormal(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.truncated_normal(shape, self.mean, self.stddev,
-                                  dtype=dtype, seed=self.seed)
+        x = K.truncated_normal(shape, self.mean, self.stddev,
+                               dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -210,12 +219,15 @@ class VarianceScaling(Initializer):
         if self.distribution == 'normal':
             # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
             stddev = np.sqrt(scale) / .87962566103423978
-            return K.truncated_normal(shape, 0., stddev,
-                                      dtype=dtype, seed=self.seed)
+            x = K.truncated_normal(shape, 0., stddev,
+                                   dtype=dtype, seed=self.seed)
         else:
             limit = np.sqrt(3. * scale)
-            return K.random_uniform(shape, -limit, limit,
-                                    dtype=dtype, seed=self.seed)
+            x = K.random_uniform(shape, -limit, limit,
+                                 dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -251,6 +263,7 @@ class Orthogonal(Initializer):
         rng = np.random
         if self.seed is not None:
             rng = np.random.RandomState(self.seed)
+            self.seed += 1
         a = rng.normal(0.0, 1.0, flat_shape)
         u, _, v = np.linalg.svd(a, full_matrices=False)
         # Pick the one with the correct shape.